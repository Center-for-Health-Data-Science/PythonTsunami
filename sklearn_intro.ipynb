{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn (`sklearn`)\n",
    "\n",
    "- Henry Webel at [NNF CPR](https://www.cpr.ku.dk/staff/rasmussen-group/?pure=en/persons/662319)\n",
    "- Python Tsumanmi 2020 at [SUND](https://healthsciences.ku.dk/)\n",
    "- Session : `Day 2, 13:00 -17.00` (Track 2)\n",
    "  - Pre-requisites: Python Intro, NumPy, minimal Pandas, matplotlib\n",
    "  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pythontsunami/teaching/blob/sklearn/sklearn_intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving the notebook in Drive\n",
    "Save a copy in your drive if you want to save your changes: `File` -> `Save a copy in Drive`\n",
    "\n",
    "\n",
    "![Save Colab Notebook in Google Drive](figures/colab_save_in_drive.png)\n",
    "\n",
    "or \n",
    "\n",
    "![Save Colab Notebook in Google Drive](figures/colab_save_in_drive_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Table of Contents in Colab**\n",
    "> Allows easier navigation\n",
    "\n",
    "![Table of content in Colab](figures/colab_toc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Scikit-learn API introdoction.\n",
    "2. If needed: Machine Learning\n",
    "3. Use-Case with different objects from scikit-learn\n",
    "    - this includes some exercises\n",
    "4. Further material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "\n",
    "Library of algorithms for Data Science with unified interface.\n",
    "\n",
    "This notebook is based on the available [tutorials](https://scikit-learn.org/stable/tutorial/index.html) which are interesting to read, but unfortunately note based on executable notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "\n",
    "- [Glossary](https://scikit-learn.org/stable/glossary.html#glossary)\n",
    "- [examples](https://github.com/scikit-learn/scikit-learn/tree/master/examples)\n",
    "- [API design for machine learning software: experiences from the scikit-learn project](https://arxiv.org/abs/1309.0238)\n",
    "- [Géron, Aurélien (2019): Hands on Machine Learning ith Scikit-Learn, Keras and TensorFlow, Vol. 2, Ch. 1- 9](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn API main principles\n",
    "> Géron (2019): 64f. and [scikit-learn-paper](https://arxiv.org/abs/1309.0238)\n",
    "\n",
    "First some theory and names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency\n",
    "- `Estimators`: Interface for building and fitting models\n",
    "    - `fit` method returns fitted models\n",
    "    - supervised: `fit(X_train, y_train)`\n",
    "    - unsupervised: `fit(X_train)`\n",
    "    - factory to produce model objects\n",
    "- `Predictors(Estimator)`: Interface for making predictions\n",
    "    - `fit`, `predict` and `score`\n",
    "    - supervised and unsupervised: `predict(X_test)`\n",
    "    - performance assessment: `score` (the higher, the better)\n",
    "    - clustering: `fit_predict` exists\n",
    "    - extends `Estimator`\n",
    "- `Transformers(Estimator)`: Interface for converting data\n",
    "    - `fit`, `transform`, and `fit_transform`\n",
    "    - extends `Estimator`\n",
    "\n",
    "    \n",
    "> Transformer which is also a predictor? Where is the difference between transform and predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composition  \n",
    "- `Pipeline` objects from a sequence of `Transformers` and a optinally a final `Predictor`\n",
    "- `FeatureUnion` objects for a two or more `Pipeline`s in parallel, yielding concatenated outputs.\n",
    "\n",
    "#### Inspection\n",
    "- learned `features_` have a underscore suffix `_`\n",
    "\n",
    "#### Sensible defaults\n",
    " - get your first models running quickly\n",
    " - sensible defaults for construction of `Estimators`\n",
    "\n",
    "> Side Note: \"A _hyperparameter_ is a parameter of a learning algorithm (not of the model).   \n",
    "> As such, it is not affected by the learning algorithm itself;   \n",
    "> it must be set prior to training and remains constant during training.\" (Géron 2019: 29)  \n",
    "> Constructor parameters of scikit-learn objects are hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website\n",
    "\n",
    "Let's have a look at the [website](https://scikit-learn.org) and see what it offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, IFrame, display\n",
    "# does not show in colab, just use the link and go to the website\n",
    "display(IFrame(src='https://scikit-learn.org', width=1024, height=1024, metadata=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Guide\n",
    "\n",
    "Some part of the [User Guide](https://scikit-learn.org/stable/user_guide.html) will be discussed.\n",
    "\n",
    "> The User Guide is an overall reference which can be followed in different orders.\n",
    "\n",
    "- [Different Estimator](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- [preprocessing data](https://scikit-learn.org/stable/data_transforms.html): `sklearn.impute`, `sklearn.preprocessing`\n",
    "- [model selection (incl. metrics)](https://scikit-learn.org/stable/model_selection.html): `sklearn.model_selection`\n",
    "- [Pipeline](https://scikit-learn.org/stable/data_transforms.html): `sklearn.pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some things to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.base\n",
    "# sklearn.base??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: CustomTransformer\n",
    "> scikit-learn is based on duck-typing, although we inherit some additional features for the interface from base classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin): \n",
    "    \"\"\"Don't use this. This is an example.\"\"\"\n",
    "    def __init__(self, my_bias=0): # no *args or **kargs\n",
    "        \"\"\"Add a bias/ intercept\"\"\"\n",
    "        self.my_bias = my_bias\n",
    "    def fit(self, X, y=None): \n",
    "        return self # nothing else to do\n",
    "    def transform(self, X):\n",
    "        return np.c_[X, np.array([self.my_bias] * len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(range(10))\n",
    "custom_transformer = CustomTransformer(my_bias=10)\n",
    "custom_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scikit-learn uses the underlying numpy.arrays of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Tutorial\n",
    "\n",
    "> Science of learning from data.  \n",
    "\n",
    "Practically this means that the computer is not entirely thought how to make decision, \n",
    "which is sometimes called rule-based (using conditional statements).\n",
    "\n",
    "1. Supervised\n",
    "    1. Regression: Continous variable prediction\n",
    "        - How old is someone?\n",
    "        - How much income can some expect?\n",
    "    2. Classification: Category prediction\n",
    "        - Disease, yes or no?\n",
    "        - disease stage: How serious is it on a scale from 0 to 4?\n",
    "2. Unsupervised: Finding groups\n",
    "    - No labels\n",
    "    - How to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt Machine Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs Regression\n",
    "\n",
    "What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Age-prediction\n",
    "> Thanks for [Sam Bradley](https://www.dtu.dk/english/service/phonebook/person?id=145074&cpid=266426&tab=0)\n",
    "telling me and [Denis Shepelin](https://www.dtu.dk/english/service/phonebook/person?id=126180&tab=2&qt=dtupublicationquery)\n",
    "telling him. There I stop the tracking:) \n",
    "\n",
    "A paper presenting age predictions based on RNA measurements did upload the data\n",
    "- [paper](https://www.sciencedirect.com/science/article/pii/S1872497317301643)\n",
    "- [data](https://zenodo.org/record/2545213/#.X43R0dAzb-g)\n",
    "\n",
    "> It's a set of features and labels\n",
    "> For first predictions you do not need to understand the biology,  \n",
    "> but to explain _odd_ things, more knowledge is most of the times helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel free to re-implement your own paper of interest \n",
    "\n",
    "> If you are interested in a paper which you have the data for, go on and try to adapt the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_train_data = \"https://zenodo.org/record/2545213/files/train_rows.csv\"\n",
    "url_test_data = \"https://zenodo.org/record/2545213/files/test_rows_labels.csv\"\n",
    "\n",
    "# additional data not used for now\n",
    "url_train_normal = \"https://zenodo.org/record/2545213/files/training_data_normal.tsv\"\n",
    "url_test_data_wo_labels = \"https://zenodo.org/record/2545213/files/test_rows.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(url_train_data, sep='\\t')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_table(url_test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_normal = pd.read_csv(url_train_normal, sep='\\t')\n",
    "# train_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_wo_label = pd.read_table(url_test_data) # tab seperated data is often tsv format\n",
    "# test_data_wo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'Age'\n",
    "\n",
    "y_train = train_data[TARGET_COLUMN]\n",
    "y_test  = test_data[TARGET_COLUMN] # pop() if you want to modify test_data inplace\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = test_data.drop(TARGET_COLUMN, axis=1)\n",
    "X_train = train_data.drop(TARGET_COLUMN, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = X_train  # from here it's easy to write a function display what you are interested in\n",
    "n_na = _df.isna().sum().sum()\n",
    "print(f\"Found # NAs: {n_na}\")\n",
    "if n_na:\n",
    "    row_with_nas = _df.isna().any(axis=1)\n",
    "    display(_df.loc[row_with_nas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = X_train.hist(figsize=(15,15), sharex=True, sharey=True)\n",
    "# _ = X_test.hist(figsize=(15,15), sharex=True, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Familiarizing with the `Age` variable\n",
    "\n",
    "> skip on the first try, as it's covered later\n",
    "\n",
    "- Check if the distribution of `Age` is the same in the predefined test and train set (there are several possibilites to do that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg = lin_reg.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Factory is replaced by fitted model, but calling fit again first erases previously fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lin_reg.predict(X_test)\n",
    "y_test_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(y_true=y_test, y_pred=y_test_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Replace the model and see if this improves your results.\n",
    "\n",
    "1. Select a different [model](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "2. Adapt only the first block from above below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple pipeline\n",
    "\n",
    "Let's add a standardiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "std_scaler.fit(X_train)\n",
    "X_train_scaled = std_scaler.transform(X_train)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg = lin_reg.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = std_scaler.transform(X_test)\n",
    "y_test_pred = lin_reg.predict(X_test_transformed)\n",
    "y_test_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(y_true=y_test, y_pred=y_test_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The result shows a property of Linear models :)\n",
    "\n",
    "Now let's build a `Pipeline` to avoid too many intermediate assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                           ('lin_reg', LinearRegression())])\n",
    "simple_pipeline = simple_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative:\n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "simple_pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = simple_pipeline.predict(X_test)\n",
    "y_test_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(y_true=y_test, y_pred=y_test_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Add an imputation step or feature selector\n",
    "\n",
    "- If you like, mask some data and add an imputer to the pipeline\n",
    "- If you have many more features, you could add a feature selector before (the `train_normal` data would have it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_keep = np.random.random(size=X_train.shape) > 0.1\n",
    "X_train.where(mask_keep) # Now X has not changed yet. Assing to a new reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excurs: Combining pipelines\n",
    "\n",
    "What if we would have an additional category?\n",
    "\n",
    "```python\n",
    "num_attribs = ['cont_var_1', 'cont_var_2']\n",
    "cat_attribs = ['cat_var_1']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', AttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "    ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "```\n",
    "\n",
    "> Check out the [notebook](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb) of Ch.2 of Géron 2019 for an extended example using a housing dataset of California, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excurs: Custom `Transformer`\n",
    "\n",
    "Create a custom Transformer adding the squared $x=x^2$ of each feature to the training data.\n",
    "\n",
    "> scikit-learn is based on duck-typing, although we inherit some additional features for the interface from base classes.\n",
    "\n",
    "##### !!! Don't use this\n",
    "To add interaction effects, please use [`sklearn.preprocessing.PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_moment=None):  # no *args or **kargs\n",
    "        self.add_moment = add_moment\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing to fit\n",
    "\n",
    "    def transform(self, X):\n",
    "        # your code here\n",
    "        return X\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_moment=True)\n",
    "extendend_data = attr_adder.transform(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can you think of a better transformations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine training and test data\n",
    "\n",
    "We generate our own training set which we will use for model selection, and a test set which will be used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, test_data])\n",
    "old_index = pd.Series(data.index)\n",
    "data.index = old_index.index\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(TARGET_COLUMN, axis=1)\n",
    "y = data[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Agenda:\n",
    "1. On the combined data set, split the data into a balanced train and test data set of 80/20 (i.e. 80% of the data goes into the training data set). \n",
    "2. Perform cross-validation\n",
    "3. Perform model-selection \n",
    "\n",
    "#### Hints\n",
    "- [model-selection tutorial](https://scikit-learn.org/stable/model_selection.html)\n",
    "\n",
    "> The aim is to get you started reading the documentation and understand the function signatures  \n",
    "> while you are able to ask as many questions as you like:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Stratification\n",
    "\n",
    "- Can you stratify the data?\n",
    "- Check if the distribution of `Age` is the same (there are several possibilites to do that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- meta-estimators `GridSearchCV` and `RandomizedSearchCV`\n",
    "- `best_estimator_` attribute\n",
    "\n",
    "- [Diabetes example](https://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "dict_scores = cross_validate(lin_reg, X, y=y, groups=y, cv=StratifiedKFold(5), scoring=None)\n",
    "dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_scores) # you can create nice tables if you work on the DataFrame further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `test_score` correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Replace `scoring=None` by other metrics by reading the documentation.\n",
    "- Extend this to several estimators and record the results\n",
    "\n",
    "> Try to google the metrics. Solution could be this [link](https://scikit-learn.org/stable/modules/model_evaluation.html)  \n",
    "> Make sure that \"greater is better\" for a score (However: You will be reminded if you forget:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring = ['metric1', 'metric2'] # replace strings\n",
    "# scoring = {'key': metric_fct}    # set key and metric_fct\n",
    "# dict_scores = cross_validate(lin_reg, X, y=y, groups=y, cv=StratifiedKFold(5), scoring=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning models\n",
    "\n",
    "`GridSearchCV` and `RandomSearchCV` on model hyperparameters.\n",
    "\n",
    "> Side Note: \"A _hyperparameter_ is a parameter of a learning algorithm (not of the model).   \n",
    "> As such, it is not affected by the learning algorithm itself;   \n",
    "> it must be set prior to training and remains constant during training.\" (Géron 2019: 29)  \n",
    "> Constructor parameters of scikit-learn objects are hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# GridSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "param_grid = {'n_estimators': [3, 10, 20], 'max_features': [2, 6, 8, 10, 13]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_reg, param_grid=param_grid, cv=5, n_jobs=3, scoring=None, return_train_score=True,\n",
    "                          verbose=1)\n",
    "grid_search.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    param_grid,\n",
    "    # then try a different set\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [10, 13]},\n",
    "  ]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_reg, param_grid=param_grid, cv=5, n_jobs=3, scoring=None, return_train_score=True,\n",
    "                          verbose=1)\n",
    "grid_search.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=rf_reg, param_grid=param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', n_jobs=4,\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "- Use `RandomizedSearchCV` \n",
    "- Try a different model if you know one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Final model\n",
    "\n",
    "The best estimator from the grid- or random-search is not yet available for training.\n",
    "\n",
    "You would need to retrain the final estimator on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence\n",
    "\n",
    "To save the model, you can use this [tutorial](https://scikit-learn.org/stable/modules/model_persistence.html).\n",
    "\n",
    "The save model to disc is self-contained, meaning that you do not need the original code to build an instance to reload the state of the model when it was safed.\n",
    "\n",
    "Model can be deployed, e.g. to Google Cloud (and probably every other one, but I have not yet tried that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "dump(lin_reg, 'lin_reg_model.joblib') \n",
    "\n",
    "clf = load('lin_reg_model.joblib')\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn API wrappers\n",
    "\n",
    "If you would like to work your scikit-learn workflow with a model from other libraries, you can use \n",
    "predeined wrappers, e.g. for DeepLearning:\n",
    "\n",
    "- [`tf.keras.wrappers.scikit_learn`](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn) (https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn) for tensorflow with keras API\n",
    "- [skorch](https://github.com/skorch-dev/skorch) for PyTorch ([PyData Berlin 2019](https://www.youtube.com/watch?v=Qbu_DCBjVEk))\n",
    "\n",
    "> What would it take to wrap any model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Image Classification \n",
    "\n",
    "Run [image-classification example](https://github.com/scikit-learn/scikit-learn/tree/master/examples/classification) and exchange the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended exercise: automated stratified Cross-Valdiation \n",
    "Goals:\n",
    "- Understanding documentation of [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) function\n",
    "- apply stratified KFold data splitting for imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Stratified Splitting is default for [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLDER_THAN = 60\n",
    "print(f\"Binary (Dummy) Variable assigning 1 if some is older than {OLDER_THAN} years old.\")\n",
    "y_binary = (y > OLDER_THAN).astype(int)\n",
    "y_binary.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will result in a imbalanced classification problem, where the aim is to predict if someone is older than `OLDER_THAN`."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python3.8 (tsunami)",
   "language": "python",
   "name": "tsunami"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc-autonumbering": true,
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
