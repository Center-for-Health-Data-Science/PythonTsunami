{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3YLOMOoHRwRR"},"outputs":[],"source":["import pandas as pd\n","import plotly.express as px"]},{"cell_type":"markdown","metadata":{"id":"k9Nxy_ic9GgE"},"source":["# Exercise\n","\n","Now that we have learned some of the basics of python, we should practice how to use this new superpower. We have here prepared a loosely guided exercise that focusses on data exploration and visualization on two example datasets, one on strokes and one on cirrhosis. You can also explore a dataset of your choosing, though the questions are prepared with the example datasets in mind.\n","\n","Here you can see the [metadata](https://www.kaggle.com/datasets/fedesoriano/cirrhosis-prediction-dataset) for the cirrhosis dataset which describes what each of the columns are. For the stroke data the meaning of the columns is more straightforward."]},{"cell_type":"markdown","metadata":{"id":"XVWSFVPK9TCe"},"source":["## 1. Data Loading\n","\n","We will start with the **stroke** dataset. You can find it on the GitHub repository under Exercise/datasets. Load the stroke data into colab by using one of the two approaches detailed below and assign it to variable name ```data```."]},{"cell_type":"markdown","metadata":{"id":"SmuuZeHpx3S_"},"source":["### 1.1 Loading the data\n","\n","**Option 1:**\n","\n","Use the pandas csv reader with a link to the data on GitHub. To do this, go to the github repository, find the stroke dataset and click on the 'raw' button. Copy that link and enter it as the file path in pandas csv reader."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR5EptJg0G1J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MXMXD20CxXHT"},"source":["... or\n","\n","**Option 2**\n","\n","Manually load the dataset into colab and then read it with the pandas csv reader. See steps below:\n","\n","1. go to the left side bar and click on the folder icon\n","2. click on data upload\n","3. select dataset from your computer\n","4. call pandas csv reader with the name of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWfDOeGB9Uw2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Gf5b1jRGxVOO"},"source":["### 1.2 First look\n","\n","Have a first look at the data. There are some neat built-in pandas functions to get an initial understanding of the data, i.e. by using the info function: `df.info()`, or the use pandas `df.describe()` function.\n","\n","Questions you might want to answer here:\n","- What different types of columns do you have?\n","- Is there a column that describes a variable that can be understood as an 'outcome' ? Which one?\n","- How many values does each variable, i.e. column, have and what are some preliminary statistics of the features? (tip: use pandas `describe` function)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-U3eWqWn9fCX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4simFEW-teti"},"source":["It helps to know which column is the outcome variable. In the stroke datasets (and many others!) the outcome variable is coded as a numerical variable. However, during analysis it should be interpreted as categorical.\n","\n","Identify the column of the outcome variable and change its type to \"category\" by using `astype()`. You can see an example in the [API reference on categorical data](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html). Remember to save your changes!\n","\n","Then, use `info()` on the dataframe again. Has it changed?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDLT6I4gtetj"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ScZ9eNl99zEN"},"source":["## 2. Exploratory analysis\n","\n","Get to know your data better. If you want to first visually inspect the data it can help to explore with some plots.\n"]},{"cell_type":"markdown","metadata":{"id":"3O904uWZtetj"},"source":["### 2.1 Violin plots and histograms\n","\n","Consider you dataframe columns that are not the outcome variable. How are the measurements distributed?\n","\n","To study the distributions we want to make **violin plots** of variables, i.e. data columns, that are numeric and **histograms** of the variables that are strings/categorical.\n","\n","To check for the data type of a column, have a look at how data types are specified in `dataframe.dtypes`. Then check the data type of each column. Remember a column is a **pandas `Series`**, so it has a `dtype` attribute instead (only dataframes have `dtypes`!).\n","\n","You can start by figuring out how to make a plot of the data in one column. Once you have that, make one plot for each column that is numeric or a string (except the outcome). This is a repetitive task, so it is ideally suited for a loop. Remember to use `fig.show()` to actually display your plots during the loop.\n","\n","**Pro version**: Some columns are not actually explanatory variables, such as a the ID column. You can identify these columns i.e. by seeing that each of their values is unique (this would be very unlikely for a measured variable). Skip them when making the plots.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llINtmry-NqA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEIouSF6tetj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XT_fy_utetk"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-d-4HSAay-xc"},"source":["### 2.2 Correlation coefficients\n","\n","Plot the correlation coefficients of all numerical features:\n","\n","1. Use the method `corr()` on the dataframe. What is the result?\n","\n","2. Now use a heatmap to show the correlation coeffcients graphically.\n","\n","3. Try some different options to make your heatmap look nicer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc3GuLRe-U6l"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQQfqB9atetk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOdMk4Qrtetk"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"v9Ez8XDCzCGW"},"source":["### 2.3 Scatter plot\n","\n","Make a scatter plot of the two variables with the highest correlation. Divide the plots by the outcome variable and add marginal plots and a trendline:\n","\n","1. Find the pair of variables that has the highest correlation with each other and make a scatter plot of them.\n","\n","2. Divide the scatter plot into two by the outcome variable. Have a look at ``facet`` and the visualization lecture if you have trouble.\n","\n","3. Add marginal distributions and a trendline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xNHY4LwAJx8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1kHLkWbtetl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyf61JtAtetm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XzWC3UdNAhdt"},"source":["## 3. Data cleaning\n","\n","Now, we switch the [cirrhosis dataset]('https://raw.githubusercontent.com/Center-for-Health-Data-Science/PythonTsunami/spring2022/Exercise/datasets/cirrhosis.csv').\n","\n","We will investigate what data is missing and try to impute it.\n","\n","A word of caution:\n","\n","Note that imputation is a __complex subject__ and whether it makes sense to do it and the method used highly depend on the data set. Sometimes, the mean of a value across all non-missing observations is a good approximation for the missing value. On the other hand, if you have a column that says whether or not the person was treated with the drug or the placebo we have no good way to guess which treatment the person received. Replacing missing values in this column with the most common value (which is that they did get the drug) will produce extremely __wrong data__ and lead you to __wrong conclusions__. Do not do that!\n"]},{"cell_type":"markdown","metadata":{"id":"uyHh_Y1Qtetm"},"source":["### 3.0 Load the data\n","\n","Load in the cirrhosis dataset using one of the two methods you used earlier for the stroke data. Change as well the outcome variable to a type \"category\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UKiUqBNtetm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"giu-oZSQtetm"},"source":["### 3.1 Missing data\n","\n","1. Use the pandas method `isnull`.\n","\n","2. Get the number of missing values per column by calling `sum()` on the result of `isnull`. Which features, i.e. columns have missing values?\n","\n","3. Make a barplot that shows the number of missing values per column.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KB_RPg-U9i8J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEbMbPHqAj2X"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3qBOePAlzIrv"},"source":["### 3.2 Omitting observations with missing values\n","\n","1. Create a subset in which you omit all patients, i.e. rows, which have missing values in any column. Take care to not overwrite the original dataframe. If you did, you can re-import it.\n","\n","2. How many observations, i.e. patients, would you be left with if you removed all missing values?\n","\n","3. How many if you only omit patients where the outcome is missing?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3rEUU7iA1La"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SbRFgmGChph"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"t1_fL5hEC2LN"},"source":["### 3.3 Effects of removing data\n","\n","We can now have a look at how removing nans effects the data.\n","\n","\n","1. First, plot the correlation coefficient between all numerical columns in the original cirrhosis dataframe. (Analogous to 2.2).\n","\n","2. Now, remake the plot for the subset where you have removed all rows with any missing data. Have the correlations changed?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5G9AqzzDSBJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9b40PElUzT-2"},"source":["### 3.4 Imputation\n","\n","Use the method `fillna()` to impute missing values in the columns **where it makes sense**. Have a look at the documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n","\n","1. A good way to impute numerical data can be i.e. the mean or median. Calculate the mean for all numerical columns.\n","\n","2. Perform the imputation.\n","\n","3. Re-make the barplot from 3.1. to check that it worked.\n","\n","4. Recalculate correlation coefficients between all numerical columns and show it in a heatmap.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SF8PoOvOcp7n"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KAI_fEIteto"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5Wq7uL3teto"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSFkAHeoteto"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["Hto297viwyzY","Gf5b1jRGxVOO","TC6ewdf5qFze","-d-4HSAay-xc","v9Ez8XDCzCGW","FxHy9gKJqtNE","3qBOePAlzIrv","nKknLSw1zOqi","9b40PElUzT-2","_CwrlBmMOzif","ATnfchJIyGKj","SS-KMKN7zeNw","feYvDsddzhUl","f8vhdvqVzjXi"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
